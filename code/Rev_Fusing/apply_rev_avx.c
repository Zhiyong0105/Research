#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <math.h>
#include <immintrin.h>
#include <pmmintrin.h>
#include "apply_rev_avx.h" 
void apply_rev_avx_mv(int k, int m, int n, double *G, double *V, int ldv, int ldg, int i)
{
__m256d  v00,  v01,  v02,  v03,  v04,  v05,  v06,  v10,  v11,  v12,  v13,  v14,  v15,  v16,  v20,  v21,  v22,  v23,  v24,  v25,  v26,  v30,  v31,  v32,  v33,  v34,  v35,  v36, gamma, sigma, tmp;
v00 = _mm256_loadu_pd(&V[i]);
v01 = _mm256_loadu_pd(&V[i + 4 * 1]);
v02 = _mm256_loadu_pd(&V[i + 4 * 2]);
v03 = _mm256_loadu_pd(&V[i + 4 * 3]);
v04 = _mm256_loadu_pd(&V[i + 4 * 4]);
v05 = _mm256_loadu_pd(&V[i + 4 * 5]);
v06 = _mm256_loadu_pd(&V[i + 4 * 6]);
v10 = _mm256_loadu_pd(&V[i + ldv]);
v11 = _mm256_loadu_pd(&V[i + ldv + 4 * 1]);
v12 = _mm256_loadu_pd(&V[i + ldv + 4 * 2]);
v13 = _mm256_loadu_pd(&V[i + ldv + 4 * 3]);
v14 = _mm256_loadu_pd(&V[i + ldv + 4 * 4]);
v15 = _mm256_loadu_pd(&V[i + ldv + 4 * 5]);
v16 = _mm256_loadu_pd(&V[i + ldv + 4 * 6]);
v20 = _mm256_loadu_pd(&V[i + 2 * ldv]);
v21 = _mm256_loadu_pd(&V[i + 2 * ldv + 4 * 1]);
v22 = _mm256_loadu_pd(&V[i + 2 * ldv + 4 * 2]);
v23 = _mm256_loadu_pd(&V[i + 2 * ldv + 4 * 3]);
v24 = _mm256_loadu_pd(&V[i + 2 * ldv + 4 * 4]);
v25 = _mm256_loadu_pd(&V[i + 2 * ldv + 4 * 5]);
v26 = _mm256_loadu_pd(&V[i + 2 * ldv + 4 * 6]);
    gamma = _mm256_broadcast_sd(&G[2 * 0 + k * ldg]);
    sigma = _mm256_broadcast_sd(&G[2 * 0 + k * ldg + 1]);
 tmp = v00;
 v00 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v10));
 v10 = _mm256_sub_pd(_mm256_mul_pd(gamma, v10), _mm256_mul_pd(sigma, tmp));
 tmp = v01;
 v01 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v11));
 v11 = _mm256_sub_pd(_mm256_mul_pd(gamma, v11), _mm256_mul_pd(sigma, tmp));
 tmp = v02;
 v02 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v12));
 v12 = _mm256_sub_pd(_mm256_mul_pd(gamma, v12), _mm256_mul_pd(sigma, tmp));
 tmp = v03;
 v03 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v13));
 v13 = _mm256_sub_pd(_mm256_mul_pd(gamma, v13), _mm256_mul_pd(sigma, tmp));
 tmp = v04;
 v04 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v14));
 v14 = _mm256_sub_pd(_mm256_mul_pd(gamma, v14), _mm256_mul_pd(sigma, tmp));
 tmp = v05;
 v05 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v15));
 v15 = _mm256_sub_pd(_mm256_mul_pd(gamma, v15), _mm256_mul_pd(sigma, tmp));
 tmp = v06;
 v06 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v16));
 v16 = _mm256_sub_pd(_mm256_mul_pd(gamma, v16), _mm256_mul_pd(sigma, tmp));
    gamma = _mm256_broadcast_sd(&G[2 * 1  + k * ldg]);
    sigma = _mm256_broadcast_sd(&G[2 * 1  + k * ldg + 1]);
 tmp = v10;
 v10 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v20));
 v20 = _mm256_sub_pd(_mm256_mul_pd(gamma, v20), _mm256_mul_pd(sigma, tmp));
 tmp = v11;
 v11 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v21));
 v21 = _mm256_sub_pd(_mm256_mul_pd(gamma, v21), _mm256_mul_pd(sigma, tmp));
 tmp = v12;
 v12 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v22));
 v22 = _mm256_sub_pd(_mm256_mul_pd(gamma, v22), _mm256_mul_pd(sigma, tmp));
 tmp = v13;
 v13 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v23));
 v23 = _mm256_sub_pd(_mm256_mul_pd(gamma, v23), _mm256_mul_pd(sigma, tmp));
 tmp = v14;
 v14 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v24));
 v24 = _mm256_sub_pd(_mm256_mul_pd(gamma, v24), _mm256_mul_pd(sigma, tmp));
 tmp = v15;
 v15 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v25));
 v25 = _mm256_sub_pd(_mm256_mul_pd(gamma, v25), _mm256_mul_pd(sigma, tmp));
 tmp = v16;
 v16 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v26));
 v26 = _mm256_sub_pd(_mm256_mul_pd(gamma, v26), _mm256_mul_pd(sigma, tmp));
    gamma = _mm256_broadcast_sd(&G[2 * 0 + (k + 1) * ldg ]);
    sigma = _mm256_broadcast_sd(&G[2 * 0 + (k + 1) * ldg  + 1]);
 tmp = v00;
 v00 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v10));
 v10 = _mm256_sub_pd(_mm256_mul_pd(gamma, v10), _mm256_mul_pd(sigma, tmp));
 tmp = v01;
 v01 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v11));
 v11 = _mm256_sub_pd(_mm256_mul_pd(gamma, v11), _mm256_mul_pd(sigma, tmp));
 tmp = v02;
 v02 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v12));
 v12 = _mm256_sub_pd(_mm256_mul_pd(gamma, v12), _mm256_mul_pd(sigma, tmp));
 tmp = v03;
 v03 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v13));
 v13 = _mm256_sub_pd(_mm256_mul_pd(gamma, v13), _mm256_mul_pd(sigma, tmp));
 tmp = v04;
 v04 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v14));
 v14 = _mm256_sub_pd(_mm256_mul_pd(gamma, v14), _mm256_mul_pd(sigma, tmp));
 tmp = v05;
 v05 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v15));
 v15 = _mm256_sub_pd(_mm256_mul_pd(gamma, v15), _mm256_mul_pd(sigma, tmp));
 tmp = v06;
 v06 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v16));
 v16 = _mm256_sub_pd(_mm256_mul_pd(gamma, v16), _mm256_mul_pd(sigma, tmp));
for (int g = 2; g < n - 1; g++)
{
v30= _mm256_loadu_pd(&V[i + (g + 1) * ldv ]);
v31= _mm256_loadu_pd(&V[i + (g + 1) * ldv + 4 * 1 ]);
v32= _mm256_loadu_pd(&V[i + (g + 1) * ldv + 4 * 2 ]);
v33= _mm256_loadu_pd(&V[i + (g + 1) * ldv + 4 * 3 ]);
v34= _mm256_loadu_pd(&V[i + (g + 1) * ldv + 4 * 4 ]);
v35= _mm256_loadu_pd(&V[i + (g + 1) * ldv + 4 * 5 ]);
v36= _mm256_loadu_pd(&V[i + (g + 1) * ldv + 4 * 6 ]);
gamma = _mm256_broadcast_sd(&G[2 * g + k * ldg]);
sigma = _mm256_broadcast_sd(&G[2 * g + k * ldg + 1]);
tmp = v20;
 v20 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v30));
 v30 = _mm256_sub_pd(_mm256_mul_pd(gamma, v30), _mm256_mul_pd(sigma, tmp));
tmp = v21;
 v21 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v31));
 v31 = _mm256_sub_pd(_mm256_mul_pd(gamma, v31), _mm256_mul_pd(sigma, tmp));
tmp = v22;
 v22 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v32));
 v32 = _mm256_sub_pd(_mm256_mul_pd(gamma, v32), _mm256_mul_pd(sigma, tmp));
tmp = v23;
 v23 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v33));
 v33 = _mm256_sub_pd(_mm256_mul_pd(gamma, v33), _mm256_mul_pd(sigma, tmp));
tmp = v24;
 v24 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v34));
 v34 = _mm256_sub_pd(_mm256_mul_pd(gamma, v34), _mm256_mul_pd(sigma, tmp));
tmp = v25;
 v25 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v35));
 v35 = _mm256_sub_pd(_mm256_mul_pd(gamma, v35), _mm256_mul_pd(sigma, tmp));
tmp = v26;
 v26 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v36));
 v36 = _mm256_sub_pd(_mm256_mul_pd(gamma, v36), _mm256_mul_pd(sigma, tmp));
gamma = _mm256_broadcast_sd(&G[2 * (g - 1) + (k + 1) * ldg]);
sigma = _mm256_broadcast_sd(&G[2 * (g - 1) + (k + 1) * ldg + 1]);
tmp = v10;
 v10 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v20));
 v20 = _mm256_sub_pd(_mm256_mul_pd(gamma, v20), _mm256_mul_pd(sigma, tmp));
tmp = v11;
 v11 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v21));
 v21 = _mm256_sub_pd(_mm256_mul_pd(gamma, v21), _mm256_mul_pd(sigma, tmp));
tmp = v12;
 v12 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v22));
 v22 = _mm256_sub_pd(_mm256_mul_pd(gamma, v22), _mm256_mul_pd(sigma, tmp));
tmp = v13;
 v13 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v23));
 v23 = _mm256_sub_pd(_mm256_mul_pd(gamma, v23), _mm256_mul_pd(sigma, tmp));
tmp = v14;
 v14 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v24));
 v24 = _mm256_sub_pd(_mm256_mul_pd(gamma, v24), _mm256_mul_pd(sigma, tmp));
tmp = v15;
 v15 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v25));
 v25 = _mm256_sub_pd(_mm256_mul_pd(gamma, v25), _mm256_mul_pd(sigma, tmp));
tmp = v16;
 v16 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v26));
 v26 = _mm256_sub_pd(_mm256_mul_pd(gamma, v26), _mm256_mul_pd(sigma, tmp));
gamma = _mm256_broadcast_sd(&G[2 * (g - 2) + (k + 2) * ldg]);
sigma = _mm256_broadcast_sd(&G[2 * (g - 2) + (k + 2) * ldg + 1]);
tmp = v00;
 v00 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v10));
 v10 = _mm256_sub_pd(_mm256_mul_pd(gamma, v10), _mm256_mul_pd(sigma, tmp));
tmp = v01;
 v01 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v11));
 v11 = _mm256_sub_pd(_mm256_mul_pd(gamma, v11), _mm256_mul_pd(sigma, tmp));
tmp = v02;
 v02 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v12));
 v12 = _mm256_sub_pd(_mm256_mul_pd(gamma, v12), _mm256_mul_pd(sigma, tmp));
tmp = v03;
 v03 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v13));
 v13 = _mm256_sub_pd(_mm256_mul_pd(gamma, v13), _mm256_mul_pd(sigma, tmp));
tmp = v04;
 v04 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v14));
 v14 = _mm256_sub_pd(_mm256_mul_pd(gamma, v14), _mm256_mul_pd(sigma, tmp));
tmp = v05;
 v05 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v15));
 v15 = _mm256_sub_pd(_mm256_mul_pd(gamma, v15), _mm256_mul_pd(sigma, tmp));
tmp = v06;
 v06 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v16));
 v16 = _mm256_sub_pd(_mm256_mul_pd(gamma, v16), _mm256_mul_pd(sigma, tmp));
_mm256_storeu_pd(&V[i + (g-2) * ldv ], v00);
_mm256_storeu_pd(&V[i + (g-2) * ldv + 4 * 1], v01);
_mm256_storeu_pd(&V[i + (g-2) * ldv + 4 * 2], v02);
_mm256_storeu_pd(&V[i + (g-2) * ldv + 4 * 3], v03);
_mm256_storeu_pd(&V[i + (g-2) * ldv + 4 * 4], v04);
_mm256_storeu_pd(&V[i + (g-2) * ldv + 4 * 5], v05);
_mm256_storeu_pd(&V[i + (g-2) * ldv + 4 * 6], v06);
v00=v10;
v01=v11;
v02=v12;
v03=v13;
v04=v14;
v05=v15;
v06=v16;
v10=v20;
v11=v21;
v12=v22;
v13=v23;
v14=v24;
v15=v25;
v16=v26;
v20=v30;
v21=v31;
v22=v32;
v23=v33;
v24=v34;
v25=v35;
v26=v36;
}
gamma = _mm256_broadcast_sd(&G[2 * (n - 2) + (k + 1) * ldg]);
sigma = _mm256_broadcast_sd(&G[2 * (n - 2) + (k + 1) * ldg + 1]);
tmp = v10;
 v10 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v20));
 v20 = _mm256_sub_pd(_mm256_mul_pd(gamma, v20), _mm256_mul_pd(sigma, tmp));
tmp = v11;
 v11 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v21));
 v21 = _mm256_sub_pd(_mm256_mul_pd(gamma, v21), _mm256_mul_pd(sigma, tmp));
tmp = v12;
 v12 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v22));
 v22 = _mm256_sub_pd(_mm256_mul_pd(gamma, v22), _mm256_mul_pd(sigma, tmp));
tmp = v13;
 v13 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v23));
 v23 = _mm256_sub_pd(_mm256_mul_pd(gamma, v23), _mm256_mul_pd(sigma, tmp));
tmp = v14;
 v14 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v24));
 v24 = _mm256_sub_pd(_mm256_mul_pd(gamma, v24), _mm256_mul_pd(sigma, tmp));
tmp = v15;
 v15 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v25));
 v25 = _mm256_sub_pd(_mm256_mul_pd(gamma, v25), _mm256_mul_pd(sigma, tmp));
tmp = v16;
 v16 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v26));
 v26 = _mm256_sub_pd(_mm256_mul_pd(gamma, v26), _mm256_mul_pd(sigma, tmp));
gamma = _mm256_broadcast_sd(&G[2 * (n - 3) + (k + 2) * ldg]);
sigma = _mm256_broadcast_sd(&G[2 * (n - 3) + (k + 2) * ldg + 1]);
tmp = v00;
 v00 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v10));
 v10 = _mm256_sub_pd(_mm256_mul_pd(gamma, v10), _mm256_mul_pd(sigma, tmp));
tmp = v01;
 v01 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v11));
 v11 = _mm256_sub_pd(_mm256_mul_pd(gamma, v11), _mm256_mul_pd(sigma, tmp));
tmp = v02;
 v02 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v12));
 v12 = _mm256_sub_pd(_mm256_mul_pd(gamma, v12), _mm256_mul_pd(sigma, tmp));
tmp = v03;
 v03 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v13));
 v13 = _mm256_sub_pd(_mm256_mul_pd(gamma, v13), _mm256_mul_pd(sigma, tmp));
tmp = v04;
 v04 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v14));
 v14 = _mm256_sub_pd(_mm256_mul_pd(gamma, v14), _mm256_mul_pd(sigma, tmp));
tmp = v05;
 v05 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v15));
 v15 = _mm256_sub_pd(_mm256_mul_pd(gamma, v15), _mm256_mul_pd(sigma, tmp));
tmp = v06;
 v06 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v16));
 v16 = _mm256_sub_pd(_mm256_mul_pd(gamma, v16), _mm256_mul_pd(sigma, tmp));
gamma = _mm256_broadcast_sd(&G[2 * (n - 2) + (k + 2) * ldg]);
sigma = _mm256_broadcast_sd(&G[2 * (n - 2) + (k + 2) * ldg + 1]);
tmp = v10;
 v10 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v20));
 v20 = _mm256_sub_pd(_mm256_mul_pd(gamma, v20), _mm256_mul_pd(sigma, tmp));
tmp = v11;
 v11 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v21));
 v21 = _mm256_sub_pd(_mm256_mul_pd(gamma, v21), _mm256_mul_pd(sigma, tmp));
tmp = v12;
 v12 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v22));
 v22 = _mm256_sub_pd(_mm256_mul_pd(gamma, v22), _mm256_mul_pd(sigma, tmp));
tmp = v13;
 v13 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v23));
 v23 = _mm256_sub_pd(_mm256_mul_pd(gamma, v23), _mm256_mul_pd(sigma, tmp));
tmp = v14;
 v14 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v24));
 v24 = _mm256_sub_pd(_mm256_mul_pd(gamma, v24), _mm256_mul_pd(sigma, tmp));
tmp = v15;
 v15 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v25));
 v25 = _mm256_sub_pd(_mm256_mul_pd(gamma, v25), _mm256_mul_pd(sigma, tmp));
tmp = v16;
 v16 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v26));
 v26 = _mm256_sub_pd(_mm256_mul_pd(gamma, v26), _mm256_mul_pd(sigma, tmp));
_mm256_storeu_pd(&V[i + (n - 3) * ldv], v00);
_mm256_storeu_pd(&V[i + (n - 3) * ldv + 4 * 1], v01);
_mm256_storeu_pd(&V[i + (n - 3) * ldv + 4 * 2], v02);
_mm256_storeu_pd(&V[i + (n - 3) * ldv + 4 * 3], v03);
_mm256_storeu_pd(&V[i + (n - 3) * ldv + 4 * 4], v04);
_mm256_storeu_pd(&V[i + (n - 3) * ldv + 4 * 5], v05);
_mm256_storeu_pd(&V[i + (n - 3) * ldv + 4 * 6], v06);
_mm256_storeu_pd(&V[i + (n - 2) * ldv], v10);
_mm256_storeu_pd(&V[i + (n - 2) * ldv + 4 * 1], v11);
_mm256_storeu_pd(&V[i + (n - 2) * ldv + 4 * 2], v12);
_mm256_storeu_pd(&V[i + (n - 2) * ldv + 4 * 3], v13);
_mm256_storeu_pd(&V[i + (n - 2) * ldv + 4 * 4], v14);
_mm256_storeu_pd(&V[i + (n - 2) * ldv + 4 * 5], v15);
_mm256_storeu_pd(&V[i + (n - 2) * ldv + 4 * 6], v16);
_mm256_storeu_pd(&V[i + (n - 1) * ldv], v20);
_mm256_storeu_pd(&V[i + (n - 1) * ldv + 4 * 1], v21);
_mm256_storeu_pd(&V[i + (n - 1) * ldv + 4 * 2], v22);
_mm256_storeu_pd(&V[i + (n - 1) * ldv + 4 * 3], v23);
_mm256_storeu_pd(&V[i + (n - 1) * ldv + 4 * 4], v24);
_mm256_storeu_pd(&V[i + (n - 1) * ldv + 4 * 5], v25);
_mm256_storeu_pd(&V[i + (n - 1) * ldv + 4 * 6], v26);
}
void apply_rev_avx_mv_seq_avx256(int k,int m, int n, double *G, double *V,int ldg)
{
__m256d  v00,  v01,  v02,  v03,  v04,  v05,  v06,  v10,  v11,  v12,  v13,  v14,  v15,  v16,  v20,  v21,  v22,  v23,  v24,  v25,  v26,  v30,  v31,  v32,  v33,  v34,  v35,  v36, gamma, sigma, tmp;
v00 = _mm256_loadu_pd(&V[0 + 0]);
v01 = _mm256_loadu_pd(&V[0 + 4]);
v02 = _mm256_loadu_pd(&V[0 + 8]);
v03 = _mm256_loadu_pd(&V[0 + 12]);
v04 = _mm256_loadu_pd(&V[0 + 16]);
v05 = _mm256_loadu_pd(&V[0 + 20]);
v06 = _mm256_loadu_pd(&V[0 + 24]);
v10 = _mm256_loadu_pd(&V[28 + 0]);
v11 = _mm256_loadu_pd(&V[28 + 4]);
v12 = _mm256_loadu_pd(&V[28 + 8]);
v13 = _mm256_loadu_pd(&V[28 + 12]);
v14 = _mm256_loadu_pd(&V[28 + 16]);
v15 = _mm256_loadu_pd(&V[28 + 20]);
v16 = _mm256_loadu_pd(&V[28 + 24]);
v20 = _mm256_loadu_pd(&V[56 + 0]);
v21 = _mm256_loadu_pd(&V[56 + 4]);
v22 = _mm256_loadu_pd(&V[56 + 8]);
v23 = _mm256_loadu_pd(&V[56 + 12]);
v24 = _mm256_loadu_pd(&V[56 + 16]);
v25 = _mm256_loadu_pd(&V[56 + 20]);
v26 = _mm256_loadu_pd(&V[56 + 24]);
    gamma = _mm256_broadcast_sd(&G[2 * 0 + k * ldg]);
    sigma = _mm256_broadcast_sd(&G[2 * 0 + k * ldg + 1]);
 tmp = v00;
 v00 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v10));
 v10 = _mm256_sub_pd(_mm256_mul_pd(gamma, v10), _mm256_mul_pd(sigma, tmp));
 tmp = v01;
 v01 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v11));
 v11 = _mm256_sub_pd(_mm256_mul_pd(gamma, v11), _mm256_mul_pd(sigma, tmp));
 tmp = v02;
 v02 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v12));
 v12 = _mm256_sub_pd(_mm256_mul_pd(gamma, v12), _mm256_mul_pd(sigma, tmp));
 tmp = v03;
 v03 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v13));
 v13 = _mm256_sub_pd(_mm256_mul_pd(gamma, v13), _mm256_mul_pd(sigma, tmp));
 tmp = v04;
 v04 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v14));
 v14 = _mm256_sub_pd(_mm256_mul_pd(gamma, v14), _mm256_mul_pd(sigma, tmp));
 tmp = v05;
 v05 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v15));
 v15 = _mm256_sub_pd(_mm256_mul_pd(gamma, v15), _mm256_mul_pd(sigma, tmp));
 tmp = v06;
 v06 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v16));
 v16 = _mm256_sub_pd(_mm256_mul_pd(gamma, v16), _mm256_mul_pd(sigma, tmp));
    gamma = _mm256_broadcast_sd(&G[2 * 1  + k * ldg]);
    sigma = _mm256_broadcast_sd(&G[2 * 1  + k * ldg + 1]);
 tmp = v10;
 v10 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v20));
 v20 = _mm256_sub_pd(_mm256_mul_pd(gamma, v20), _mm256_mul_pd(sigma, tmp));
 tmp = v11;
 v11 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v21));
 v21 = _mm256_sub_pd(_mm256_mul_pd(gamma, v21), _mm256_mul_pd(sigma, tmp));
 tmp = v12;
 v12 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v22));
 v22 = _mm256_sub_pd(_mm256_mul_pd(gamma, v22), _mm256_mul_pd(sigma, tmp));
 tmp = v13;
 v13 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v23));
 v23 = _mm256_sub_pd(_mm256_mul_pd(gamma, v23), _mm256_mul_pd(sigma, tmp));
 tmp = v14;
 v14 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v24));
 v24 = _mm256_sub_pd(_mm256_mul_pd(gamma, v24), _mm256_mul_pd(sigma, tmp));
 tmp = v15;
 v15 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v25));
 v25 = _mm256_sub_pd(_mm256_mul_pd(gamma, v25), _mm256_mul_pd(sigma, tmp));
 tmp = v16;
 v16 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v26));
 v26 = _mm256_sub_pd(_mm256_mul_pd(gamma, v26), _mm256_mul_pd(sigma, tmp));
    gamma = _mm256_broadcast_sd(&G[2 * 0 + (k + 1) * ldg ]);
    sigma = _mm256_broadcast_sd(&G[2 * 0 + (k + 1) * ldg  + 1]);
 tmp = v00;
 v00 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v10));
 v10 = _mm256_sub_pd(_mm256_mul_pd(gamma, v10), _mm256_mul_pd(sigma, tmp));
 tmp = v01;
 v01 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v11));
 v11 = _mm256_sub_pd(_mm256_mul_pd(gamma, v11), _mm256_mul_pd(sigma, tmp));
 tmp = v02;
 v02 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v12));
 v12 = _mm256_sub_pd(_mm256_mul_pd(gamma, v12), _mm256_mul_pd(sigma, tmp));
 tmp = v03;
 v03 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v13));
 v13 = _mm256_sub_pd(_mm256_mul_pd(gamma, v13), _mm256_mul_pd(sigma, tmp));
 tmp = v04;
 v04 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v14));
 v14 = _mm256_sub_pd(_mm256_mul_pd(gamma, v14), _mm256_mul_pd(sigma, tmp));
 tmp = v05;
 v05 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v15));
 v15 = _mm256_sub_pd(_mm256_mul_pd(gamma, v15), _mm256_mul_pd(sigma, tmp));
 tmp = v06;
 v06 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v16));
 v16 = _mm256_sub_pd(_mm256_mul_pd(gamma, v16), _mm256_mul_pd(sigma, tmp));
for (int g = 2; g < n - 1; g++)
{
v30 = _mm256_loadu_pd(&V[(g + 1) * 28 + 0]);
v31 = _mm256_loadu_pd(&V[(g + 1) * 28 + 4]);
v32 = _mm256_loadu_pd(&V[(g + 1) * 28 + 8]);
v33 = _mm256_loadu_pd(&V[(g + 1) * 28 + 12]);
v34 = _mm256_loadu_pd(&V[(g + 1) * 28 + 16]);
v35 = _mm256_loadu_pd(&V[(g + 1) * 28 + 20]);
v36 = _mm256_loadu_pd(&V[(g + 1) * 28 + 24]);
gamma =_mm256_broadcast_sd(&G[2 * g + k * ldg]);
sigma = _mm256_broadcast_sd(&G[2 * g + k * ldg + 1]);
tmp = v20;
 v20 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v30));
 v30 = _mm256_sub_pd(_mm256_mul_pd(gamma, v30), _mm256_mul_pd(sigma, tmp));
tmp = v21;
 v21 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v31));
 v31 = _mm256_sub_pd(_mm256_mul_pd(gamma, v31), _mm256_mul_pd(sigma, tmp));
tmp = v22;
 v22 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v32));
 v32 = _mm256_sub_pd(_mm256_mul_pd(gamma, v32), _mm256_mul_pd(sigma, tmp));
tmp = v23;
 v23 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v33));
 v33 = _mm256_sub_pd(_mm256_mul_pd(gamma, v33), _mm256_mul_pd(sigma, tmp));
tmp = v24;
 v24 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v34));
 v34 = _mm256_sub_pd(_mm256_mul_pd(gamma, v34), _mm256_mul_pd(sigma, tmp));
tmp = v25;
 v25 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v35));
 v35 = _mm256_sub_pd(_mm256_mul_pd(gamma, v35), _mm256_mul_pd(sigma, tmp));
tmp = v26;
 v26 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v36));
 v36 = _mm256_sub_pd(_mm256_mul_pd(gamma, v36), _mm256_mul_pd(sigma, tmp));
gamma =_mm256_broadcast_sd(&G[2 * (g - 1) + (k + 1) * ldg]);
sigma = _mm256_broadcast_sd(&G[2 * (g - 1) + (k + 1) * ldg + 1]);
tmp = v10;
 v10 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v20));
 v20 = _mm256_sub_pd(_mm256_mul_pd(gamma, v20), _mm256_mul_pd(sigma, tmp));
tmp = v11;
 v11 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v21));
 v21 = _mm256_sub_pd(_mm256_mul_pd(gamma, v21), _mm256_mul_pd(sigma, tmp));
tmp = v12;
 v12 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v22));
 v22 = _mm256_sub_pd(_mm256_mul_pd(gamma, v22), _mm256_mul_pd(sigma, tmp));
tmp = v13;
 v13 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v23));
 v23 = _mm256_sub_pd(_mm256_mul_pd(gamma, v23), _mm256_mul_pd(sigma, tmp));
tmp = v14;
 v14 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v24));
 v24 = _mm256_sub_pd(_mm256_mul_pd(gamma, v24), _mm256_mul_pd(sigma, tmp));
tmp = v15;
 v15 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v25));
 v25 = _mm256_sub_pd(_mm256_mul_pd(gamma, v25), _mm256_mul_pd(sigma, tmp));
tmp = v16;
 v16 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v26));
 v26 = _mm256_sub_pd(_mm256_mul_pd(gamma, v26), _mm256_mul_pd(sigma, tmp));
gamma =_mm256_broadcast_sd(&G[2 * (g - 2) + (k + 2) * ldg]);
sigma = _mm256_broadcast_sd(&G[2 * (g - 2) + (k + 2) * ldg + 1]);
tmp = v00;
 v00 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v10));
 v10 = _mm256_sub_pd(_mm256_mul_pd(gamma, v10), _mm256_mul_pd(sigma, tmp));
tmp = v01;
 v01 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v11));
 v11 = _mm256_sub_pd(_mm256_mul_pd(gamma, v11), _mm256_mul_pd(sigma, tmp));
tmp = v02;
 v02 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v12));
 v12 = _mm256_sub_pd(_mm256_mul_pd(gamma, v12), _mm256_mul_pd(sigma, tmp));
tmp = v03;
 v03 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v13));
 v13 = _mm256_sub_pd(_mm256_mul_pd(gamma, v13), _mm256_mul_pd(sigma, tmp));
tmp = v04;
 v04 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v14));
 v14 = _mm256_sub_pd(_mm256_mul_pd(gamma, v14), _mm256_mul_pd(sigma, tmp));
tmp = v05;
 v05 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v15));
 v15 = _mm256_sub_pd(_mm256_mul_pd(gamma, v15), _mm256_mul_pd(sigma, tmp));
tmp = v06;
 v06 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v16));
 v16 = _mm256_sub_pd(_mm256_mul_pd(gamma, v16), _mm256_mul_pd(sigma, tmp));
_mm256_storeu_pd(&V[28 * (g - 2)], v00);
_mm256_storeu_pd(&V[28 * (g - 2) + 4 * 1], v01);
_mm256_storeu_pd(&V[28 * (g - 2) + 4 * 2], v02);
_mm256_storeu_pd(&V[28 * (g - 2) + 4 * 3], v03);
_mm256_storeu_pd(&V[28 * (g - 2) + 4 * 4], v04);
_mm256_storeu_pd(&V[28 * (g - 2) + 4 * 5], v05);
_mm256_storeu_pd(&V[28 * (g - 2) + 4 * 6], v06);
v00=v10;
v01=v11;
v02=v12;
v03=v13;
v04=v14;
v05=v15;
v06=v16;
v10=v20;
v11=v21;
v12=v22;
v13=v23;
v14=v24;
v15=v25;
v16=v26;
v20=v30;
v21=v31;
v22=v32;
v23=v33;
v24=v34;
v25=v35;
v26=v36;
}
gamma = _mm256_broadcast_sd(&G[2 * (n - 2) + (k + 1) * ldg]);
sigma = _mm256_broadcast_sd(&G[2 * (n - 2) + (k + 1) * ldg + 1]);
tmp = v10;
 v10 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v20));
 v20 = _mm256_sub_pd(_mm256_mul_pd(gamma, v20), _mm256_mul_pd(sigma, tmp));
tmp = v11;
 v11 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v21));
 v21 = _mm256_sub_pd(_mm256_mul_pd(gamma, v21), _mm256_mul_pd(sigma, tmp));
tmp = v12;
 v12 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v22));
 v22 = _mm256_sub_pd(_mm256_mul_pd(gamma, v22), _mm256_mul_pd(sigma, tmp));
tmp = v13;
 v13 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v23));
 v23 = _mm256_sub_pd(_mm256_mul_pd(gamma, v23), _mm256_mul_pd(sigma, tmp));
tmp = v14;
 v14 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v24));
 v24 = _mm256_sub_pd(_mm256_mul_pd(gamma, v24), _mm256_mul_pd(sigma, tmp));
tmp = v15;
 v15 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v25));
 v25 = _mm256_sub_pd(_mm256_mul_pd(gamma, v25), _mm256_mul_pd(sigma, tmp));
tmp = v16;
 v16 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v26));
 v26 = _mm256_sub_pd(_mm256_mul_pd(gamma, v26), _mm256_mul_pd(sigma, tmp));
gamma = _mm256_broadcast_sd(&G[2 * (n - 3) + (k + 2) * ldg]);
sigma = _mm256_broadcast_sd(&G[2 * (n - 3) + (k + 2) * ldg + 1]);
tmp = v00;
 v00 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v10));
 v10 = _mm256_sub_pd(_mm256_mul_pd(gamma, v10), _mm256_mul_pd(sigma, tmp));
tmp = v01;
 v01 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v11));
 v11 = _mm256_sub_pd(_mm256_mul_pd(gamma, v11), _mm256_mul_pd(sigma, tmp));
tmp = v02;
 v02 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v12));
 v12 = _mm256_sub_pd(_mm256_mul_pd(gamma, v12), _mm256_mul_pd(sigma, tmp));
tmp = v03;
 v03 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v13));
 v13 = _mm256_sub_pd(_mm256_mul_pd(gamma, v13), _mm256_mul_pd(sigma, tmp));
tmp = v04;
 v04 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v14));
 v14 = _mm256_sub_pd(_mm256_mul_pd(gamma, v14), _mm256_mul_pd(sigma, tmp));
tmp = v05;
 v05 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v15));
 v15 = _mm256_sub_pd(_mm256_mul_pd(gamma, v15), _mm256_mul_pd(sigma, tmp));
tmp = v06;
 v06 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v16));
 v16 = _mm256_sub_pd(_mm256_mul_pd(gamma, v16), _mm256_mul_pd(sigma, tmp));
gamma = _mm256_broadcast_sd(&G[2 * (n - 2) + (k + 2) * ldg]);
sigma = _mm256_broadcast_sd(&G[2 * (n - 2) + (k + 2) * ldg + 1]);
tmp = v10;
 v10 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v20));
 v20 = _mm256_sub_pd(_mm256_mul_pd(gamma, v20), _mm256_mul_pd(sigma, tmp));
tmp = v11;
 v11 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v21));
 v21 = _mm256_sub_pd(_mm256_mul_pd(gamma, v21), _mm256_mul_pd(sigma, tmp));
tmp = v12;
 v12 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v22));
 v22 = _mm256_sub_pd(_mm256_mul_pd(gamma, v22), _mm256_mul_pd(sigma, tmp));
tmp = v13;
 v13 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v23));
 v23 = _mm256_sub_pd(_mm256_mul_pd(gamma, v23), _mm256_mul_pd(sigma, tmp));
tmp = v14;
 v14 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v24));
 v24 = _mm256_sub_pd(_mm256_mul_pd(gamma, v24), _mm256_mul_pd(sigma, tmp));
tmp = v15;
 v15 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v25));
 v25 = _mm256_sub_pd(_mm256_mul_pd(gamma, v25), _mm256_mul_pd(sigma, tmp));
tmp = v16;
 v16 = _mm256_add_pd(_mm256_mul_pd(gamma, tmp), _mm256_mul_pd(sigma, v26));
 v26 = _mm256_sub_pd(_mm256_mul_pd(gamma, v26), _mm256_mul_pd(sigma, tmp));
_mm256_storeu_pd(&V[28 * (n - 3)], v00);
_mm256_storeu_pd(&V[28 * (n - 3)+ 4 * 1], v01);
_mm256_storeu_pd(&V[28 * (n - 3)+ 4 * 2], v02);
_mm256_storeu_pd(&V[28 * (n - 3)+ 4 * 3], v03);
_mm256_storeu_pd(&V[28 * (n - 3)+ 4 * 4], v04);
_mm256_storeu_pd(&V[28 * (n - 3)+ 4 * 5], v05);
_mm256_storeu_pd(&V[28 * (n - 3)+ 4 * 6], v06);
_mm256_storeu_pd(&V[28 * (n - 2)], v10);
_mm256_storeu_pd(&V[28 * (n - 2)+ 4 * 1], v11);
_mm256_storeu_pd(&V[28 * (n - 2)+ 4 * 2], v12);
_mm256_storeu_pd(&V[28 * (n - 2)+ 4 * 3], v13);
_mm256_storeu_pd(&V[28 * (n - 2)+ 4 * 4], v14);
_mm256_storeu_pd(&V[28 * (n - 2)+ 4 * 5], v15);
_mm256_storeu_pd(&V[28 * (n - 2)+ 4 * 6], v16);
_mm256_storeu_pd(&V[28 * (n - 1)], v20);
_mm256_storeu_pd(&V[28 * (n - 1)+ 4 * 1], v21);
_mm256_storeu_pd(&V[28 * (n - 1)+ 4 * 2], v22);
_mm256_storeu_pd(&V[28 * (n - 1)+ 4 * 3], v23);
_mm256_storeu_pd(&V[28 * (n - 1)+ 4 * 4], v24);
_mm256_storeu_pd(&V[28 * (n - 1)+ 4 * 5], v25);
_mm256_storeu_pd(&V[28 * (n - 1)+ 4 * 6], v26);
}
void apply_rev_avx_mv_seq(int k,int m, int n, double *G, double *V,int ldg)
{
__m512d  v00,  v01,  v02,  v03,  v04,  v05,  v06,  v10,  v11,  v12,  v13,  v14,  v15,  v16,  v20,  v21,  v22,  v23,  v24,  v25,  v26,  v30,  v31,  v32,  v33,  v34,  v35,  v36, gamma, sigma, tmp;
v00 = _mm512_loadu_pd(&V[0 + 0]);
v01 = _mm512_loadu_pd(&V[0 + 8]);
v02 = _mm512_loadu_pd(&V[0 + 16]);
v03 = _mm512_loadu_pd(&V[0 + 24]);
v04 = _mm512_loadu_pd(&V[0 + 32]);
v05 = _mm512_loadu_pd(&V[0 + 40]);
v06 = _mm512_loadu_pd(&V[0 + 48]);
v10 = _mm512_loadu_pd(&V[56 + 0]);
v11 = _mm512_loadu_pd(&V[56 + 8]);
v12 = _mm512_loadu_pd(&V[56 + 16]);
v13 = _mm512_loadu_pd(&V[56 + 24]);
v14 = _mm512_loadu_pd(&V[56 + 32]);
v15 = _mm512_loadu_pd(&V[56 + 40]);
v16 = _mm512_loadu_pd(&V[56 + 48]);
v20 = _mm512_loadu_pd(&V[112 + 0]);
v21 = _mm512_loadu_pd(&V[112 + 8]);
v22 = _mm512_loadu_pd(&V[112 + 16]);
v23 = _mm512_loadu_pd(&V[112 + 24]);
v24 = _mm512_loadu_pd(&V[112 + 32]);
v25 = _mm512_loadu_pd(&V[112 + 40]);
v26 = _mm512_loadu_pd(&V[112 + 48]);
    gamma = _mm512_set1_pd(G[2 * 0 + k * ldg]);
    sigma = _mm512_set1_pd(G[2 * 0 + k * ldg + 1]);
 tmp = v00;
 v00 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v10));
 v10 = _mm512_sub_pd(_mm512_mul_pd(gamma, v10), _mm512_mul_pd(sigma, tmp));
 tmp = v01;
 v01 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v11));
 v11 = _mm512_sub_pd(_mm512_mul_pd(gamma, v11), _mm512_mul_pd(sigma, tmp));
 tmp = v02;
 v02 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v12));
 v12 = _mm512_sub_pd(_mm512_mul_pd(gamma, v12), _mm512_mul_pd(sigma, tmp));
 tmp = v03;
 v03 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v13));
 v13 = _mm512_sub_pd(_mm512_mul_pd(gamma, v13), _mm512_mul_pd(sigma, tmp));
 tmp = v04;
 v04 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v14));
 v14 = _mm512_sub_pd(_mm512_mul_pd(gamma, v14), _mm512_mul_pd(sigma, tmp));
 tmp = v05;
 v05 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v15));
 v15 = _mm512_sub_pd(_mm512_mul_pd(gamma, v15), _mm512_mul_pd(sigma, tmp));
 tmp = v06;
 v06 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v16));
 v16 = _mm512_sub_pd(_mm512_mul_pd(gamma, v16), _mm512_mul_pd(sigma, tmp));
    gamma = _mm512_set1_pd(G[2 * 1  + k * ldg]);
    sigma = _mm512_set1_pd(G[2 * 1  + k * ldg + 1]);
 tmp = v10;
 v10 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v20));
 v20 = _mm512_sub_pd(_mm512_mul_pd(gamma, v20), _mm512_mul_pd(sigma, tmp));
 tmp = v11;
 v11 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v21));
 v21 = _mm512_sub_pd(_mm512_mul_pd(gamma, v21), _mm512_mul_pd(sigma, tmp));
 tmp = v12;
 v12 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v22));
 v22 = _mm512_sub_pd(_mm512_mul_pd(gamma, v22), _mm512_mul_pd(sigma, tmp));
 tmp = v13;
 v13 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v23));
 v23 = _mm512_sub_pd(_mm512_mul_pd(gamma, v23), _mm512_mul_pd(sigma, tmp));
 tmp = v14;
 v14 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v24));
 v24 = _mm512_sub_pd(_mm512_mul_pd(gamma, v24), _mm512_mul_pd(sigma, tmp));
 tmp = v15;
 v15 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v25));
 v25 = _mm512_sub_pd(_mm512_mul_pd(gamma, v25), _mm512_mul_pd(sigma, tmp));
 tmp = v16;
 v16 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v26));
 v26 = _mm512_sub_pd(_mm512_mul_pd(gamma, v26), _mm512_mul_pd(sigma, tmp));
    gamma = _mm512_set1_pd(G[2 * 0 + (k + 1) * ldg ]);
    sigma = _mm512_set1_pd(G[2 * 0 + (k + 1) * ldg  + 1]);
 tmp = v00;
 v00 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v10));
 v10 = _mm512_sub_pd(_mm512_mul_pd(gamma, v10), _mm512_mul_pd(sigma, tmp));
 tmp = v01;
 v01 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v11));
 v11 = _mm512_sub_pd(_mm512_mul_pd(gamma, v11), _mm512_mul_pd(sigma, tmp));
 tmp = v02;
 v02 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v12));
 v12 = _mm512_sub_pd(_mm512_mul_pd(gamma, v12), _mm512_mul_pd(sigma, tmp));
 tmp = v03;
 v03 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v13));
 v13 = _mm512_sub_pd(_mm512_mul_pd(gamma, v13), _mm512_mul_pd(sigma, tmp));
 tmp = v04;
 v04 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v14));
 v14 = _mm512_sub_pd(_mm512_mul_pd(gamma, v14), _mm512_mul_pd(sigma, tmp));
 tmp = v05;
 v05 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v15));
 v15 = _mm512_sub_pd(_mm512_mul_pd(gamma, v15), _mm512_mul_pd(sigma, tmp));
 tmp = v06;
 v06 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v16));
 v16 = _mm512_sub_pd(_mm512_mul_pd(gamma, v16), _mm512_mul_pd(sigma, tmp));
for (int g = 2; g < n - 1; g++)
{
v30 = _mm512_loadu_pd(&V[(g + 1) * 56 + 0]);
v31 = _mm512_loadu_pd(&V[(g + 1) * 56 + 8]);
v32 = _mm512_loadu_pd(&V[(g + 1) * 56 + 16]);
v33 = _mm512_loadu_pd(&V[(g + 1) * 56 + 24]);
v34 = _mm512_loadu_pd(&V[(g + 1) * 56 + 32]);
v35 = _mm512_loadu_pd(&V[(g + 1) * 56 + 40]);
v36 = _mm512_loadu_pd(&V[(g + 1) * 56 + 48]);
gamma = _mm512_set1_pd(G[2 * g + k * ldg]);
sigma = _mm512_set1_pd(G[2 * g + k * ldg + 1]);
tmp = v20;
 v20 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v30));
 v30 = _mm512_sub_pd(_mm512_mul_pd(gamma, v30), _mm512_mul_pd(sigma, tmp));
tmp = v21;
 v21 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v31));
 v31 = _mm512_sub_pd(_mm512_mul_pd(gamma, v31), _mm512_mul_pd(sigma, tmp));
tmp = v22;
 v22 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v32));
 v32 = _mm512_sub_pd(_mm512_mul_pd(gamma, v32), _mm512_mul_pd(sigma, tmp));
tmp = v23;
 v23 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v33));
 v33 = _mm512_sub_pd(_mm512_mul_pd(gamma, v33), _mm512_mul_pd(sigma, tmp));
tmp = v24;
 v24 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v34));
 v34 = _mm512_sub_pd(_mm512_mul_pd(gamma, v34), _mm512_mul_pd(sigma, tmp));
tmp = v25;
 v25 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v35));
 v35 = _mm512_sub_pd(_mm512_mul_pd(gamma, v35), _mm512_mul_pd(sigma, tmp));
tmp = v26;
 v26 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v36));
 v36 = _mm512_sub_pd(_mm512_mul_pd(gamma, v36), _mm512_mul_pd(sigma, tmp));
gamma = _mm512_set1_pd(G[2 * (g - 1) + (k + 1) * ldg]);
sigma = _mm512_set1_pd(G[2 * (g - 1) + (k + 1) * ldg + 1]);
tmp = v10;
 v10 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v20));
 v20 = _mm512_sub_pd(_mm512_mul_pd(gamma, v20), _mm512_mul_pd(sigma, tmp));
tmp = v11;
 v11 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v21));
 v21 = _mm512_sub_pd(_mm512_mul_pd(gamma, v21), _mm512_mul_pd(sigma, tmp));
tmp = v12;
 v12 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v22));
 v22 = _mm512_sub_pd(_mm512_mul_pd(gamma, v22), _mm512_mul_pd(sigma, tmp));
tmp = v13;
 v13 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v23));
 v23 = _mm512_sub_pd(_mm512_mul_pd(gamma, v23), _mm512_mul_pd(sigma, tmp));
tmp = v14;
 v14 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v24));
 v24 = _mm512_sub_pd(_mm512_mul_pd(gamma, v24), _mm512_mul_pd(sigma, tmp));
tmp = v15;
 v15 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v25));
 v25 = _mm512_sub_pd(_mm512_mul_pd(gamma, v25), _mm512_mul_pd(sigma, tmp));
tmp = v16;
 v16 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v26));
 v26 = _mm512_sub_pd(_mm512_mul_pd(gamma, v26), _mm512_mul_pd(sigma, tmp));
gamma = _mm512_set1_pd(G[2 * (g - 2) + (k + 2) * ldg]);
sigma = _mm512_set1_pd(G[2 * (g - 2) + (k + 2) * ldg + 1]);
tmp = v00;
 v00 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v10));
 v10 = _mm512_sub_pd(_mm512_mul_pd(gamma, v10), _mm512_mul_pd(sigma, tmp));
tmp = v01;
 v01 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v11));
 v11 = _mm512_sub_pd(_mm512_mul_pd(gamma, v11), _mm512_mul_pd(sigma, tmp));
tmp = v02;
 v02 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v12));
 v12 = _mm512_sub_pd(_mm512_mul_pd(gamma, v12), _mm512_mul_pd(sigma, tmp));
tmp = v03;
 v03 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v13));
 v13 = _mm512_sub_pd(_mm512_mul_pd(gamma, v13), _mm512_mul_pd(sigma, tmp));
tmp = v04;
 v04 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v14));
 v14 = _mm512_sub_pd(_mm512_mul_pd(gamma, v14), _mm512_mul_pd(sigma, tmp));
tmp = v05;
 v05 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v15));
 v15 = _mm512_sub_pd(_mm512_mul_pd(gamma, v15), _mm512_mul_pd(sigma, tmp));
tmp = v06;
 v06 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v16));
 v16 = _mm512_sub_pd(_mm512_mul_pd(gamma, v16), _mm512_mul_pd(sigma, tmp));
_mm512_storeu_pd(&V[56 * (g - 2)], v00);
_mm512_storeu_pd(&V[56 * (g - 2) + 8 * 1], v01);
_mm512_storeu_pd(&V[56 * (g - 2) + 8 * 2], v02);
_mm512_storeu_pd(&V[56 * (g - 2) + 8 * 3], v03);
_mm512_storeu_pd(&V[56 * (g - 2) + 8 * 4], v04);
_mm512_storeu_pd(&V[56 * (g - 2) + 8 * 5], v05);
_mm512_storeu_pd(&V[56 * (g - 2) + 8 * 6], v06);
v00=v10;
v01=v11;
v02=v12;
v03=v13;
v04=v14;
v05=v15;
v06=v16;
v10=v20;
v11=v21;
v12=v22;
v13=v23;
v14=v24;
v15=v25;
v16=v26;
v20=v30;
v21=v31;
v22=v32;
v23=v33;
v24=v34;
v25=v35;
v26=v36;
}
gamma = _mm512_set1_pd(G[2 * (n - 2) + (k + 1) * ldg]);
sigma = _mm512_set1_pd(G[2 * (n - 2) + (k + 1) * ldg + 1]);
tmp = v10;
 v10 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v20));
 v20 = _mm512_sub_pd(_mm512_mul_pd(gamma, v20), _mm512_mul_pd(sigma, tmp));
tmp = v11;
 v11 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v21));
 v21 = _mm512_sub_pd(_mm512_mul_pd(gamma, v21), _mm512_mul_pd(sigma, tmp));
tmp = v12;
 v12 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v22));
 v22 = _mm512_sub_pd(_mm512_mul_pd(gamma, v22), _mm512_mul_pd(sigma, tmp));
tmp = v13;
 v13 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v23));
 v23 = _mm512_sub_pd(_mm512_mul_pd(gamma, v23), _mm512_mul_pd(sigma, tmp));
tmp = v14;
 v14 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v24));
 v24 = _mm512_sub_pd(_mm512_mul_pd(gamma, v24), _mm512_mul_pd(sigma, tmp));
tmp = v15;
 v15 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v25));
 v25 = _mm512_sub_pd(_mm512_mul_pd(gamma, v25), _mm512_mul_pd(sigma, tmp));
tmp = v16;
 v16 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v26));
 v26 = _mm512_sub_pd(_mm512_mul_pd(gamma, v26), _mm512_mul_pd(sigma, tmp));
gamma = _mm512_set1_pd(G[2 * (n - 3) + (k + 2) * ldg]);
sigma = _mm512_set1_pd(G[2 * (n - 3) + (k + 2) * ldg + 1]);
tmp = v00;
 v00 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v10));
 v10 = _mm512_sub_pd(_mm512_mul_pd(gamma, v10), _mm512_mul_pd(sigma, tmp));
tmp = v01;
 v01 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v11));
 v11 = _mm512_sub_pd(_mm512_mul_pd(gamma, v11), _mm512_mul_pd(sigma, tmp));
tmp = v02;
 v02 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v12));
 v12 = _mm512_sub_pd(_mm512_mul_pd(gamma, v12), _mm512_mul_pd(sigma, tmp));
tmp = v03;
 v03 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v13));
 v13 = _mm512_sub_pd(_mm512_mul_pd(gamma, v13), _mm512_mul_pd(sigma, tmp));
tmp = v04;
 v04 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v14));
 v14 = _mm512_sub_pd(_mm512_mul_pd(gamma, v14), _mm512_mul_pd(sigma, tmp));
tmp = v05;
 v05 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v15));
 v15 = _mm512_sub_pd(_mm512_mul_pd(gamma, v15), _mm512_mul_pd(sigma, tmp));
tmp = v06;
 v06 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v16));
 v16 = _mm512_sub_pd(_mm512_mul_pd(gamma, v16), _mm512_mul_pd(sigma, tmp));
gamma = _mm512_set1_pd(G[2 * (n - 2) + (k + 2) * ldg]);
sigma = _mm512_set1_pd(G[2 * (n - 2) + (k + 2) * ldg + 1]);
tmp = v10;
 v10 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v20));
 v20 = _mm512_sub_pd(_mm512_mul_pd(gamma, v20), _mm512_mul_pd(sigma, tmp));
tmp = v11;
 v11 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v21));
 v21 = _mm512_sub_pd(_mm512_mul_pd(gamma, v21), _mm512_mul_pd(sigma, tmp));
tmp = v12;
 v12 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v22));
 v22 = _mm512_sub_pd(_mm512_mul_pd(gamma, v22), _mm512_mul_pd(sigma, tmp));
tmp = v13;
 v13 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v23));
 v23 = _mm512_sub_pd(_mm512_mul_pd(gamma, v23), _mm512_mul_pd(sigma, tmp));
tmp = v14;
 v14 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v24));
 v24 = _mm512_sub_pd(_mm512_mul_pd(gamma, v24), _mm512_mul_pd(sigma, tmp));
tmp = v15;
 v15 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v25));
 v25 = _mm512_sub_pd(_mm512_mul_pd(gamma, v25), _mm512_mul_pd(sigma, tmp));
tmp = v16;
 v16 = _mm512_add_pd(_mm512_mul_pd(gamma, tmp), _mm512_mul_pd(sigma, v26));
 v26 = _mm512_sub_pd(_mm512_mul_pd(gamma, v26), _mm512_mul_pd(sigma, tmp));
_mm512_storeu_pd(&V[56 * (n - 3)], v00);
_mm512_storeu_pd(&V[56 * (n - 3)+ 8 * 1], v01);
_mm512_storeu_pd(&V[56 * (n - 3)+ 8 * 2], v02);
_mm512_storeu_pd(&V[56 * (n - 3)+ 8 * 3], v03);
_mm512_storeu_pd(&V[56 * (n - 3)+ 8 * 4], v04);
_mm512_storeu_pd(&V[56 * (n - 3)+ 8 * 5], v05);
_mm512_storeu_pd(&V[56 * (n - 3)+ 8 * 6], v06);
_mm512_storeu_pd(&V[56 * (n - 2)], v10);
_mm512_storeu_pd(&V[56 * (n - 2)+ 8 * 1], v11);
_mm512_storeu_pd(&V[56 * (n - 2)+ 8 * 2], v12);
_mm512_storeu_pd(&V[56 * (n - 2)+ 8 * 3], v13);
_mm512_storeu_pd(&V[56 * (n - 2)+ 8 * 4], v14);
_mm512_storeu_pd(&V[56 * (n - 2)+ 8 * 5], v15);
_mm512_storeu_pd(&V[56 * (n - 2)+ 8 * 6], v16);
_mm512_storeu_pd(&V[56 * (n - 1)], v20);
_mm512_storeu_pd(&V[56 * (n - 1)+ 8 * 1], v21);
_mm512_storeu_pd(&V[56 * (n - 1)+ 8 * 2], v22);
_mm512_storeu_pd(&V[56 * (n - 1)+ 8 * 3], v23);
_mm512_storeu_pd(&V[56 * (n - 1)+ 8 * 4], v24);
_mm512_storeu_pd(&V[56 * (n - 1)+ 8 * 5], v25);
_mm512_storeu_pd(&V[56 * (n - 1)+ 8 * 6], v26);
}
